{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b73c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "\"\"\"\n",
    "pip install pinecone\n",
    "pip install torch torchvision torchaudio\n",
    "pip install sentence-transformers\n",
    "pip install pypdf\n",
    "pip install openai\n",
    "pip install --upgrade --quiet langchain langchain-community langchain_openai\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from pypdf import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b81dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dotevn\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the CSV file containing the video transcripts. \n",
    "clean_text = pd.read_csv(\"clean_text.csv\")\n",
    "print(clean_text.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41debac0",
   "metadata": {},
   "source": [
    "## Creating and initializing the vector database index using Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31383f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the API keys\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY =os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "# Set as environment variables\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
    "\n",
    "print(\"OpenAI API key loaded and set as environment variable.\")\n",
    "print(\"Pinecone API key loaded and set as environment variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d578ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7935f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pc.delete_index(\"indexfp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"indexfp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefdd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,    \n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ),\n",
    "    )\n",
    "    # wait until the index is ready\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to the index and check stats are all zeros\n",
    "index = pc.Index(index_name)\n",
    "print(index.describe_index_stats()) #initialize the index, and insure the stats are all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# load the retriever model from huggingface model hub\n",
    "retriever = SentenceTransformer(\n",
    "    \"BAAI/bge-base-en-v1.5\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Wait for the index to be ready\n",
    "print(f\"Waiting for index '{index_name}' to be ready...\")\n",
    "while True:\n",
    "    try:\n",
    "        # describe_index returns an IndexDescription object which has a status field\n",
    "        index_description = pc.describe_index(index_name)\n",
    "        if index_description.status.ready:\n",
    "            print(f\"Index '{index_name}' is ready.\")\n",
    "            break\n",
    "        else:\n",
    "            # Index is not ready yet, print current state and wait\n",
    "            print(f\"Index '{index_name}' is not ready yet. Status: {index_description.status.state}. Waiting...\")\n",
    "            time.sleep(10) # Wait for 10 seconds before checking again\n",
    "    except Exception as e:\n",
    "        # Catch potential errors if index is still being created/initialized by Pinecone\n",
    "        print(f\"Error checking index status: {e}. Retrying in 10 seconds...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "# Ensure the stats are all zeros (for a freshly created/empty index)\n",
    "# Use index.describe_index_stats() to get the current statistics\n",
    "stats = index.describe_index_stats()\n",
    "\n",
    "# Check if the total_vector_count is 0\n",
    "if stats.total_vector_count == 0:\n",
    "    print(f\"Index '{index_name}' is empty (total_vector_count: {stats.total_vector_count}).\")\n",
    "    print(f\"Dimension: {stats.dimension}\")\n",
    "else:\n",
    "    print(f\"WARNING: Index '{index_name}' is not empty. Current stats:\")\n",
    "    print(stats) # Print full stats if not empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56725270",
   "metadata": {},
   "source": [
    "## Initializing YT Embedding chunks of the YT video transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_text.copy()\n",
    "\n",
    "# unique, stable id per chunk\n",
    "df[\"id\"] = df.apply(\n",
    "    lambda r: f\"{r['video_id']}_{int(r['chunk_number'])}\", axis=1\n",
    ")\n",
    "\n",
    "# text to embed\n",
    "df[\"passage_text\"] = df[\"Processed_Text_chunk\"]\n",
    "\n",
    "# optional metadata for your template\n",
    "df[\"article_title\"] = df[\"video_id\"]\n",
    "df[\"section_title\"] = df[\"chunk_number\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dc699",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "print(f\"\\nStarting embedding generation and upsert for {len(df)} documents into Pinecone index '{index_name}'...\")\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Upserting batches to Pinecone\"):\n",
    "    i_end = min(i + batch_size, len(df))\n",
    "    batch_df = df.iloc[i:i_end]\n",
    "\n",
    "    batch_texts = batch_df[\"passage_text\"].tolist()\n",
    "\n",
    "    # document embeddings using the retriever model\n",
    "    batch_embeddings = retriever.encode(\n",
    "        batch_texts,\n",
    "        show_progress_bar=False,\n",
    "        device=device\n",
    "    ).tolist()\n",
    "\n",
    "    vectors_for_batch = []\n",
    "\n",
    "    for j, row in enumerate(batch_df.itertuples(index=False)):\n",
    "        doc_id = str(row.id)\n",
    "\n",
    "        metadata = {\n",
    "            \"source\": \"transcript\",\n",
    "            \"video_id\": row.video_id,\n",
    "            \"passage_text\": row.passage_text,\n",
    "            # \"snippet\": row.passage_text[:500],\n",
    "        }\n",
    "\n",
    "        vectors_for_batch.append({\n",
    "            \"id\": doc_id,\n",
    "            \"values\": batch_embeddings[j],\n",
    "            \"metadata\": metadata,\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        index.upsert(vectors=vectors_for_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError upserting batch {i}-{i_end}: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nFinished generating embeddings and upserting all batches to Pinecone.\")\n",
    "\n",
    "final_index_stats = index.describe_index_stats()\n",
    "if \"\" in final_index_stats.namespaces:\n",
    "    print(f\"Total vectors in index: {final_index_stats.namespaces[''].vector_count}\")\n",
    "else:\n",
    "    print(\"Total vectors in index: 0 (No default namespace found).\")\n",
    "print(final_index_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68bc99",
   "metadata": {},
   "source": [
    "## Loading PDF and Initializing their embedding chunks in the Vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  Load PDFs into a pandas DataFrame\n",
    "\n",
    "def load_pdfs_to_dataframe(pdf_folder=\"files\"):\n",
    "    \"\"\"\n",
    "    Reads all PDF files in a folder and loads them into a pandas DataFrame.\n",
    "    Columns: pdf_file, page_number, text\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    if not os.path.isdir(pdf_folder):\n",
    "        print(f\"Folder '{pdf_folder}' does not exist.\")\n",
    "        return pd.DataFrame(columns=[\"pdf_file\", \"page_number\", \"text\"])\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "\n",
    "        try:\n",
    "            reader = PdfReader(pdf_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {pdf_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            text = page.extract_text() or \"\"\n",
    "            rows.append({\n",
    "                \"pdf_file\": pdf_file,\n",
    "                \"page_number\": i,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# %%  Chunking helpers for PDF text\n",
    "\n",
    "def chunk_text(text, chunk_size=400, overlap=50):\n",
    "    \"\"\"\n",
    "    Word based chunking with overlap.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk_words = words[start:end]\n",
    "        if not chunk_words:\n",
    "            break\n",
    "        chunks.append(\" \".join(chunk_words))\n",
    "        start += max(chunk_size - overlap, 1)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_pdf_dataframe(pdf_df, chunk_size=400, overlap=50):\n",
    "    \"\"\"\n",
    "    Takes the page based pdf_df and returns a new DataFrame\n",
    "    with one row per chunk.\n",
    "\n",
    "    Columns:\n",
    "      pdf_file\n",
    "      page_number\n",
    "      chunk_index\n",
    "      passage_text\n",
    "    \"\"\"\n",
    "    chunk_rows = []\n",
    "\n",
    "    for _, row in pdf_df.iterrows():\n",
    "        text = row[\"text\"] or \"\"\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_rows.append({\n",
    "                \"pdf_file\": row[\"pdf_file\"],\n",
    "                \"page_number\": row[\"page_number\"],\n",
    "                \"chunk_index\": idx,\n",
    "                \"passage_text\": chunk,\n",
    "            })\n",
    "\n",
    "    if not chunk_rows:\n",
    "        return pd.DataFrame(columns=[\"pdf_file\", \"page_number\", \"chunk_index\", \"passage_text\"])\n",
    "\n",
    "    return pd.DataFrame(chunk_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c13516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  Upsert PDF chunks into the same Pinecone index\n",
    "\n",
    "# Load and chunk PDFs\n",
    "pdf_df = load_pdfs_to_dataframe(\"files\")  # change folder name if needed\n",
    "pdf_chunks_df = chunk_pdf_dataframe(pdf_df, chunk_size=400, overlap=80)\n",
    "\n",
    "print(f\"\\nFound {len(pdf_chunks_df)} PDF chunks to upsert into Pinecone index '{index_name}'...\")\n",
    "\n",
    "batch_size_pdf = 64\n",
    "\n",
    "for i in tqdm(range(0, len(pdf_chunks_df), batch_size_pdf), desc=\"Upserting PDF batches to Pinecone\"):\n",
    "    i_end = min(i + batch_size_pdf, len(pdf_chunks_df))\n",
    "    batch_df = pdf_chunks_df.iloc[i:i_end]\n",
    "\n",
    "    batch_texts = batch_df[\"passage_text\"].tolist()\n",
    "\n",
    "    # same retriever, same embedding space\n",
    "    batch_embeddings = retriever.encode(\n",
    "        batch_texts,\n",
    "        show_progress_bar=False,\n",
    "        device=device\n",
    "    ).tolist()\n",
    "\n",
    "    vectors_for_batch = []\n",
    "\n",
    "    for j, row in enumerate(batch_df.itertuples(index=False)):\n",
    "        # unique ID so it does not collide with transcript IDs\n",
    "        doc_id = f\"pdf_{row.pdf_file}_{row.page_number}_{row.chunk_index}\"\n",
    "\n",
    "        metadata = {\n",
    "            \"source\": \"pdf\",\n",
    "            \"pdf_file\": row.pdf_file,\n",
    "            \"page_number\": int(row.page_number),\n",
    "            \"chunk_index\": int(row.chunk_index),\n",
    "            \"passage_text\": row.passage_text,\n",
    "        }\n",
    "\n",
    "        vectors_for_batch.append({\n",
    "            \"id\": doc_id,\n",
    "            \"values\": batch_embeddings[j],\n",
    "            \"metadata\": metadata,\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        index.upsert(vectors=vectors_for_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError upserting PDF batch {i}-{i_end}: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"\\nFinished generating embeddings and upserting all PDF chunks to Pinecone.\")\n",
    "\n",
    "# Show updated stats including PDFs\n",
    "final_index_stats = index.describe_index_stats()\n",
    "if \"\" in final_index_stats.namespaces:\n",
    "    print(f\"Total vectors in index (transcripts + PDFs): {final_index_stats.namespaces[''].vector_count}\")\n",
    "else:\n",
    "    print(\"Total vectors in index: 0 (No default namespace found).\")\n",
    "print(final_index_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3e172",
   "metadata": {},
   "source": [
    "## Creating a tool to generate the user request and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "LAST_MOCK_TEST = None  # holds the last mock exam text\n",
    "EXAM_STATE = {\n",
    "    \"in_exam\": False,\n",
    "    \"start_time\": None,\n",
    "}\n",
    "\n",
    "def generate_answer(question, retrieved_chunks):\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    prompt = f\"\"\"You are a expert tutor for the CompTIA Security+ SY-701 certification course. Coaching students for their certification preparation.\n",
    "Use the following rules:\n",
    "\n",
    "1) Start by using the context below whenever it is relevant. Quote or paraphrase it where needed.\n",
    "2) If the context does not fully answer the question (for example, the user asks for a 60-day plan, extra practice tests, or “top 10 acronyms”), then:\n",
    "   - Use your own knowledge to extend or create a helpful answer,\n",
    "   - Keep everything aligned with Security+ SY0-701 exam preparation.\n",
    "3) If the question is clearly unrelated to Security+, say: \"I don't know based on this course.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "       model=\"gpt-4o-mini\",\n",
    "       messages=[\n",
    "           {\n",
    "               \"role\": \"system\",\n",
    "               \"content\": \"You are a Security+ SY0-701 tutor. Prefer using course transcripts when they contain relevant information. When transcripts do not contain everything needed, extend the answer using your own knowledge while keeping it aligned with Security+ SY0-701.\"\n",
    "           },\n",
    "           {\"role\": \"user\", \"content\": prompt},\n",
    "       ],\n",
    "       temperature=0,\n",
    "       max_tokens=300,\n",
    "   )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec46383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Pinecone query funtion \n",
    "def query_pinecone(query, top_k=15):\n",
    "    # embed the question\n",
    "    q_emb = retriever.encode([query], convert_to_tensor=False)[0].tolist()\n",
    "\n",
    "    # query the Pinecone index\n",
    "    result = index.query(\n",
    "        vector=q_emb,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Pinecone matching funtion \n",
    "def extract_chunks(query_result):\n",
    "    chunks = []\n",
    "    for match in query_result[\"matches\"]:\n",
    "        chunks.append(match[\"metadata\"][\"passage_text\"])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824371d",
   "metadata": {},
   "source": [
    "## Defining the funtion to search and deliver the results to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, top_k=15):\n",
    "    \"\"\"\n",
    "    Main brain of the chatbot.\n",
    "\n",
    "    Capabilities:\n",
    "      1) General Security+ questions -> RAG over Pinecone + LLM.\n",
    "      2) Generate a mock exam / full exam simulation when user asks for it.\n",
    "      3) After a mock exam is generated, answer requests like:\n",
    "         - \"What is the answer and explanation to question 20?\"\n",
    "         - \"Give me all the correct answers.\"\n",
    "         - \"Here are my answers, please grade me.\"\n",
    "      4) When grading, mention how many minutes the learner used since exam start.\n",
    "    \"\"\"\n",
    "    global LAST_MOCK_TEST, EXAM_STATE\n",
    "\n",
    "    lower_q = question.lower()\n",
    "\n",
    "    # 1) User asks for a mock test or full exam simulation\n",
    "    exam_request_phrases = [\n",
    "        \"mock test\",\n",
    "        \"mock exam\",\n",
    "        \"practice exam\",\n",
    "        \"full exam\",\n",
    "        \"exam simulation\",\n",
    "        \"real exam\",\n",
    "        \"real time exam\",\n",
    "        \"real-time exam\",  # in case the user writes it like this\n",
    "    ]\n",
    "\n",
    "    if any(phrase in lower_q for phrase in exam_request_phrases):\n",
    "        prompt = f\"\"\"You are a Security+ SY0-701 tutor.\n",
    "\n",
    "Create a realistic mock exam similar in style and difficulty to the CompTIA Security+ SY0-701 certification exam.\n",
    "\n",
    "Requirements:\n",
    "- Use multiple choice questions only.\n",
    "- Aim for around 20 to 30 questions that could reasonably take about 90 minutes.\n",
    "- Cover a balanced mix of domains (threats, architecture, implementation, operations, governance, cryptography).\n",
    "- Output ONLY the questions and the answer options (A, B, C, D).\n",
    "- DO NOT include the correct answers or explanations in this response.\n",
    "- At the end, invite the learner to answer the questions and then ask for the answer key or grading.\n",
    "\n",
    "Learner request:\n",
    "{question}\n",
    "\n",
    "Now output the mock exam questions:\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        mock_text = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Ready-to-copy answer template for the user\n",
    "        answer_template = (\n",
    "            \"My answers are 1: , 2: , 3: , 4: , 5: , 6: , 7: , 8: , 9: , 10: , \"\n",
    "            \"11: , 12: , 13: , 14: , 15: , 16: , 17: , 18: , 19: , 20: . \"\n",
    "            \"Please grade my answers and tell me the score.\"\n",
    "        )\n",
    "\n",
    "        mock_text_with_tip = (\n",
    "            f\"{mock_text}\\n\\n\"\n",
    "            \"When you are ready, you can send your answers in a single message. \"\n",
    "            \"For example, you can copy this template format for your answers:\\n\"\n",
    "            f\"{answer_template}\"\n",
    "        )\n",
    "\n",
    "        # store only the pure mock exam for grading\n",
    "        LAST_MOCK_TEST = mock_text\n",
    "        EXAM_STATE[\"in_exam\"] = True\n",
    "        EXAM_STATE[\"start_time\"] = datetime.utcnow()\n",
    "\n",
    "        return mock_text_with_tip\n",
    "    \n",
    "    # 2) User asks for answers or grading for the last mock exam\n",
    "    wants_answers_or_grading = any(\n",
    "        word in lower_q\n",
    "        for word in [\n",
    "            \"answer\",\n",
    "            \"answers\",\n",
    "            \"solution\",\n",
    "            \"solutions\",\n",
    "            \"correct option\",\n",
    "            \"correct options\",\n",
    "            \"answer key\",\n",
    "            \"grade\",\n",
    "            \"score\",\n",
    "            \"check my answers\",\n",
    "            \"mark my answers\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if LAST_MOCK_TEST is not None and wants_answers_or_grading:\n",
    "        # compute how many minutes have passed since exam start (if in exam mode)\n",
    "        elapsed_minutes_text = \"\"\n",
    "        if EXAM_STATE[\"in_exam\"] and EXAM_STATE[\"start_time\"] is not None:\n",
    "            elapsed = datetime.utcnow() - EXAM_STATE[\"start_time\"]\n",
    "            used_minutes = int(elapsed.total_seconds() // 60)\n",
    "            elapsed_minutes_text = f\"The learner took about {used_minutes} minutes between exam start and this request.\"\n",
    "\n",
    "        prompt = f\"\"\"You are a Security+ SY0-701 tutor.\n",
    "\n",
    "Below is a mock exam that was previously given to the learner:\n",
    "\n",
    "--- MOCK EXAM START ---\n",
    "{LAST_MOCK_TEST}\n",
    "--- MOCK EXAM END ---\n",
    "\n",
    "{elapsed_minutes_text}\n",
    "\n",
    "The learner now says:\n",
    "\"{question}\"\n",
    "\n",
    "Follow these rules:\n",
    "\n",
    "1) If the learner asks for ALL the answers or for an \"answer key\":\n",
    "   - Provide a numbered list of correct answers for every question.\n",
    "   - For each question, show:\n",
    "     - The correct option (A, B, C, or D).\n",
    "     - A short explanation.\n",
    "\n",
    "2) If the learner refers to a specific question number, such as \"question 20\":\n",
    "   - Provide the correct option and a short explanation ONLY for those question numbers.\n",
    "\n",
    "3) If the learner provides their own answers (for example \"1:B, 2:C, 3:A...\"):\n",
    "   - Compare their answers to the correct ones.\n",
    "   - Show which questions are correct and which are incorrect.\n",
    "   - For incorrect ones, show the correct answer.\n",
    "   - For incorrect ones, also provide a brief explanation.\n",
    "   - Provide an overall score at the end (for example \"You scored 16 out of 20\").\n",
    "\n",
    "Be very explicit about which question number you are referring to in each line.\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "\n",
    "        # after grading, end the exam session but keep LAST_MOCK_TEST\n",
    "        EXAM_STATE[\"in_exam\"] = False\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    # 3) Normal RAG behaviour for all other questions (general chatbot mode)\n",
    "\n",
    "    # 1. retrieve from Pinecone\n",
    "    results = query_pinecone(question, top_k=top_k)\n",
    "    chunks = extract_chunks(results)\n",
    "\n",
    "    # 2. build context for the LLM\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "\n",
    "    prompt = f\"\"\"You are a Security+ SY0-701 tutor.\n",
    "\n",
    "Use the context below when it is helpful.\n",
    "If the context contains partial information, extend the answer using your own Security+ SY0-701 knowledge.\n",
    "If the question is clearly unrelated to Security+ study or exam preparation, say: \"I don't know based on this course.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=1400,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
