{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727409cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Dependencies\n",
    "\n",
    "#pip install langchain langchain-community langchain_openai\n",
    "#pip install pandas\n",
    "#pip install pinecone\n",
    "#pip install sentence-transformers\n",
    "#pip install -U \"langchain\" \"langchain-core\" \"langchain-openai\"\n",
    "#pip install langchain-core\n",
    "#pip install langchain-classic\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4dd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from RAG_system import ask \n",
    "from langchain_classic.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967133a",
   "metadata": {},
   "source": [
    "## Defining RAG Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba036263",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def ask_rag_tool(question: str, previous_answer: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Use the RAG system (Pinecone + course transcripts) as the primary source,\n",
    "    and let the model extend the answer when the context is not enough.\n",
    "    If previous_answer is provided, treat this as a follow-up and include that\n",
    "    answer as context when forming the query.\n",
    "    \"\"\"\n",
    "    if previous_answer:\n",
    "        full_question = f\"{previous_answer}\\n\\nUser follow up: {question}\"\n",
    "    else:\n",
    "        full_question = question\n",
    "    return ask(full_question, top_k=15)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "tools = [ask_rag_tool]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a Security+ SY0 701 tutor.\\n\"\n",
    "            \"You must always answer by calling the tool ask_rag_tool exactly once.\\n\"\n",
    "            \"When you receive the output from ask_rag_tool, your final answer to the user \"\n",
    "            \"must be exactly that output, verbatim, without shortening, summarizing, or \"\n",
    "            \"dropping any part of it. Do not rewrite or compress the tool output. \"\n",
    "            \"Just return it as the answer.\\n\"\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e536880",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the funtion for the RAG tool retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56591e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Call the agent_executor and always return the full RAG answer string.\n",
    "    Instead of using result[\"output\"], take the tool output from intermediate_steps.\n",
    "    \"\"\"\n",
    "    result = agent_executor.invoke({\"input\": query})\n",
    "\n",
    "    # LangChain stores (AgentAction, tool_output) pairs here\n",
    "    steps = result.get(\"intermediate_steps\", [])\n",
    "\n",
    "    if steps:\n",
    "        # Take the last tool call output\n",
    "        last_action, last_output = steps[-1]\n",
    "        answer = last_output\n",
    "    else:\n",
    "        # Fallback to the agent final output if no tool was called\n",
    "        answer = result.get(\"output\", \"\")\n",
    "\n",
    "    # Normalize to string\n",
    "    if isinstance(answer, str):\n",
    "        text = answer\n",
    "    elif hasattr(answer, \"content\"):\n",
    "        text = str(answer.content)\n",
    "    else:\n",
    "        text = str(answer)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    print(\"Raw Answer from chat():\", repr(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"RAG chatbot with tools and memory. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        q = input(\"Ask question or write quit: \")\n",
    "        if q.strip().lower() in {\"exit\", \"quit\"}:\n",
    "            break\n",
    "        answer = chat(q)\n",
    "        print(f\"Bot: {answer}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
